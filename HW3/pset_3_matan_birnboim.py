# -*- coding: utf-8 -*-
"""Pset_3_Matan_Birnboim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NjrSbT5b4Jsd9T8wmw6NSEFjmGnEsTTs

This executable notebook will help you complete Pset 3.

If you haven't used Colab before, it's very similar to Jupyter / IPython / R Notebooks: cells containing Python code can be interactively run, and their outputs will be interpolated into this document. If you haven't used any such software before, we recommend [taking a quick tour of Colab](https://colab.research.google.com/notebooks/basic_features_overview.ipynb).

---

Now, a few Colab-specific things to note about execution before we get started:

- Google offers free compute (including GPU compute!) on this notebook, but *only for a limited time*. Your session will be automatically closed after 12 hours. That means you'll want to finish within 12 hours of starting, or make sure to save your intermediate work (see the next bullet).
- You can save and write files from this notebook, but they are *not guaranteed to persist*. For this reason, we'll mount a Google Drive account and write to that Drive when any files need to be kept permanently.
- You should keep this tab open until you're completely finished with the notebook. If you close the tab, your session will be marked as "Idle" and may be terminated.

# Getting started

**First**, make a copy of this notebook so you can make your own changes. Click *File -> Save a copy in Drive*.

### What you need to do

Read through this notebook and execute each cell in sequence, making modifications and adding code where necessary. You should execute all of the code as instructed, and make sure to write code or textual responses wherever the text **TODO** shows up in text and code cells.

When you're finished, choose *File -> Download .ipynb*. You will upload this `.ipynb` file as part of your submission.

## 1) Logistic Regression

### Background: logistic regression for binomial ordering preferences

We'll walk you through the example of logistic regression that we covered during class, where we took a dataset of binomial expressions and inferred the relative strengths of the short-before-long and frequent-before-infrequent ordering preferences.  We first load the dataset, which consists of a number of binomial expressions each of which was observed once in a sample of the Brown corpus, in the order given in the dataset.  In this dataset, `Syl` and `Freq` respectively denote whether the observed ordering matches the preference (an entry of `1`), violates the preference (an entry of `-1`), or is irrelevant for the preference (an entry of `0`, indicating that either ordering would satisfy the preference).  `Percept` indicates matching or violation of the perceptual markedness preference, and `Response` is a dummy variable whose value is always `1`, which we will use in fitting the logistic regression model.
"""

import statsmodels.api as sm
import pandas as pd
import numpy as np
d = pd.read_csv("https://gist.githubusercontent.com/scaperex/b577698c3f497f43df453d28c9c580fd/raw/6480ca2da71a42f75b490baa5387773f3aeb72e1/single_count_binomials.txt",header=0,sep=" ")
d

"""Recall that logistic regression involves the following equations for predictors $\{X_i\}$:

$\eta = \sum_i \beta_i X_i$ (the **linear predictor**)

$P($outcome$=$success$) =\frac{e^{\eta}}{1 + e^{\eta}}$ (outcomes are Bernoulli distributed around the mean resulting from a logistic transformation of the linear predictor)

We have two predictors: $X_1$ is `Syl` and $X_2$ is `Freq`.  We use the `statsmodels` Python package to fit this logistic regression model to our dataset and infer the parameter weights $\{\beta_i\}$, which correspond to the preference strengths.  In `statsmodels`, as in most software packages implementing logistic regression, it is a convention that the numeric coding of the outcome or response is `1` for "success" and `0` otherwise.  Also as in most software packages for logistic regression, we use matrix formats to represent the response & predictors: so if there are $M$ predictors and $N$ observations, then the predictor set is represented as an $M \times N$ matrix and the response variable is represented as a $1 \times N$ matrix (effectively a length-$N$ vector).  We split our dataset into predictor and response matrices, and then fit a logistic regression model.

(In `statsmodels`, as with many statistical software packages, logistic regression is implemented as a special case of the more general framework of generalized linear models (GLMs), which is why the third line of the below cell looks the way it does.  We won't be covering GLMs in this class, but you may encounter them in other statistics classes or, perhaps less likely, in machine-learning classes.)
"""

x = d[["Syl","Freq"]]
y = d[["Response"]]
m = sm.GLM(y,x,family=sm.families.Binomial()) # first argument is response, second argument is predictor matrix, third argument says this is logistic regression
m_results = m.fit()
print(m_results.summary())

"""The `coef` results of `0.48` for `Syl` and `0.40` match those we covered in class.

How well are we able to predict the ordering of a binomial we haven't previously seen will occur in?  To estimate this, we'll create a random 80/20 train/test split of our binomials data, estimate our logistic regression weights using the training dataset, and then see how often our prediction is successful ($P(success)>0.5$ for the observed ordering of the test-set binomial).  First we create our train/test split:
"""

import math, random
N = d.shape[0]
N_train = math.floor(N*4/5)
idx = list(range(N))
random.seed(3) # so that results will be reproducible from run to run
random.shuffle(idx)
idx_train = idx[0:N_train]
idx_test = idx[N_train:N]
print(idx_train)
print(idx_test)
d_train = d.iloc[idx_train]
d_test = d.iloc[idx_test]
print(d_train)

"""And now we train a logistic model on only the training set, predict success probability for the observed binomials in the test set, and see how often we "succeed":"""

x_train = d_train[["Syl","Freq"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial()) # first argument is response, second argument is predictor matrix, third argument says this is logistic regression
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["Syl","Freq"]]
y_predicted = m_results.predict(x_test)
np.mean(y_predicted>0.5)

"""The answer: apparently somewhat better than 50/50 chance!

Another measure of how well a model fits a dataset is the log-likelihood it assigns to the data.  
"""

sum(np.log(y_predicted)) # large (less negative) values indicate better fit.

"""### Accuracy and Log-likelihood

Note, in the binary classification case, accuracy is defined as:
$$Acc = \frac{1}{N} \sum_i  \mathbb{1}\{\hat{y_i}==y_i\}$$
Where

$$\hat{y_i} = 1 \ \textbf{if} \ \hat{p(x_i)}>0.5 \  \textbf{else} \ 0$$


And log likelihood is defined as:

$$L = \sum_i [y_i\cdot log(p(x_i)) + (1-y_i) \cdot log(1-p(x_i))] $$

### A new application of logistic regression: the dative alternation

The work you need to do for this pset involves applying logistic regression to a new case, the **dative alternation**, which we studied in a previous pset.  We will use the `dative` dataset from Bresnan et al. (2007).  First we load the dataset:
"""

dat = pd.read_csv("https://gist.githubusercontent.com/scaperex/278815a736401d36021aa9fe31b9a0cb/raw/cf338a8cf745fa5820c4ea97af682d265bc1a34f/dative-alternation.csv")
dat

"""We see that it uses text values for some of the variables we are interested in (the response variable `RealizationOfRecipient`, and the variables expressing length and pronominality of theme and object).  We create numeric versions of these variables, arbitrarily coding a double object outcome as `1` ("success") and a prepositional dative outcome as `0`."""

dat["Response"] = [1 if x =="NP" else 0 for x in dat["RealizationOfRecipient"]]
dat["RecPro"] = [1 if x =="pronominal" else 0 for x in dat["PronomOfRec"]]
dat["ThemePro"] = [1 if x =="pronominal" else 0 for x in dat["PronomOfTheme"]]
dat[["RealizationOfRecipient","Response","PronomOfRec","RecPro","PronomOfTheme","ThemePro"]]

## TODO: create numeric variables for PronomOfTheme
dat["logLengthOfTheme"]=np.log2(dat["LengthOfTheme"])
dat["logLengthOfRecipient"]=np.log2(dat["LengthOfRecipient"])

"""To capture the possibility of an overall preference for one construction or the other, we add an "intercept" term to the logistic regression model, by creating a new `Dummy` variable in the data frame.  We then fit a baseline model using only the intercept and find that there is an overall majority preference for the **DO** realization in this dataset (the intercept's fitted weight is greater than 0).  We also see that the intercept-only model simply recapitulates the sample mean."""

dat["Dummy"] = 1
x = dat[["Dummy"]]
y = dat[["Response"]]
m = sm.GLM(y,x,family=sm.families.Binomial()) # first argument is response, second argument is predictor matrix, third argument says this is logistic regression
m_results = m.fit()
print(m_results.summary())
print("Predicted proportion of DO outcomes based on fitted intercept-only model:", round(np.mean(m_results.predict(x)),4))
print("Proportion of data with DO outcome:", round(np.mean(y["Response"]),4)) # same as model-predicted proportion

"""**Task:** In the below code boxes, complete the five parts of the problem specified in the pset PDF."""

## TODO: define and implement an 80/20 train/test random split of the "dative" dataset
#just like we did before
N = dat.shape[0]
N_train = math.floor(N*4/5)
idx = list(range(N))
random.seed(3)
random.shuffle(idx)
idx_train = idx[0:N_train]
idx_test = idx[N_train:N]
print(idx_train)
print(idx_test)
d_train = dat.iloc[idx_train]
d_test = dat.iloc[idx_test]
print(d_train)

def log_likelihood(predictions, labels):
    values = [y*np.log(predicted)+(1-y)*np.log(1-predicted) for y, predicted in zip(labels.values, predictions)]
    return sum(values)[0]

def accuracy(predictions, labels):
    labelled_predictions = [1 if prediction > 0.5 else 0 for prediction in predictions]
    correct = [1 if prediction == label else 0 for prediction, label in zip(labelled_predictions, labels)]
    return sum(correct)/len(labels)

## TODO: Fit a logistic regression model to the training set that uses only recipient pronominality
##       and an intercept term.
##       What is its classification accuracy on the held-out test dataset? How about its log-likelihood?
x_train = d_train[["RecPro","Dummy"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial())
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["RecPro","Dummy"]]
y_test = d_test[["Response"]]
y_predicted = m_results.predict(x_test)
print("Predicted proportion of DO outcomes based on fitted intercept and recipient pronominality model:", round(np.mean(y_predicted), 4))
print("Proportion of data with DO outcome:", round(np.mean(dat["Response"]),4))
print(f'Classification Accuracy: {round(accuracy(y_predicted, y_test.values), 4)}')
print(f'Log-Likelihood {round(log_likelihood(y_predicted, y_test), 4)}')

"""**TODO:** interpretation goes here.

The findings suggest that the model's suitability is significant. The significantly low p-value and the coefficient of the variable both indicate that Recipient Pronominality is a reliable indicator for DO. Furthermore, the predicted proportion of DO outcomes (0.735) closely resembled the actual proportion (0.739), although this measure alone may not be highly informative. However, the classification accuracy ultimately reached only 0.61, surpassing random performance but not achieving exceptional results. Additionally, the log-likelihood yielded a considerably negative value, suggesting that the fit may not be as strong as initially anticipated.
"""

## TODO: Add theme pronominality as a predictor to the model and see whether that improves the
##       model’s predictive power as assessed by held-out classification accuracy and log-likelihood.
x_train = d_train[["RecPro","ThemePro","Dummy"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial())
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["RecPro","ThemePro","Dummy"]]
y_test = d_test[["Response"]]
y_predicted = m_results.predict(x_test)
print("Predicted proportion of DO outcomes based on fitted intercept and recipient pronominality model:", round(np.mean(y_predicted), 4))
print("Proportion of data with DO outcome:", round(np.mean(dat["Response"]),4))
print(f'Classification Accuracy: {round(accuracy(y_predicted, y_test.values), 4)}')
print(f'Log-Likelihood {round(log_likelihood(y_predicted, y_test), 4)}')

"""**TODO:** interpretation goes here.

These results demonstrate a substantial improvement. The p-values once again affirm the significance of both variables in the regression model. Notably, the accuracy has shown a notable improvement, reaching 0.787, which is statistically significant. Moreover, the log-likelihood of this model has improved significantly, indicating a stronger fit. Based on these findings, we can confidently conclude that the Theme Pronominality variable plays a valuable role in enhancing the predictive power and fit of the model.
"""

## TODO: Determine whether additionally adding theme and recipient length (in number of words)
##       to the model further improves fit. Try both raw length or log-transformed length.
##       Which gives better performance?

#Length
x_train = d_train[["RecPro", "ThemePro","LengthOfTheme","LengthOfRecipient","Dummy"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial())
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["RecPro", "ThemePro","LengthOfTheme","LengthOfRecipient","Dummy"]]
y_test = d_test[["Response"]]
y_predicted = m_results.predict(x_test)
print("Predicted proportion of DO outcomes based on fitted intercept and recipient pronominality model:", round(np.mean(y_predicted), 4))
print("Proportion of data with DO outcome:", round(np.mean(dat["Response"]),4))
print(f'Classification Accuracy: {round(accuracy(y_predicted, y_test.values), 4)}')
print(f'Log-Likelihood {round(log_likelihood(y_predicted, y_test), 4)}')

#Log-transformed length
x_train = d_train[["RecPro", "ThemePro","logLengthOfTheme","logLengthOfRecipient","Dummy"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial())
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["RecPro", "ThemePro","logLengthOfTheme","logLengthOfRecipient","Dummy"]]
y_test = d_test[["Response"]]
y_predicted = m_results.predict(x_test)
print("Predicted proportion of DO outcomes based on fitted intercept and recipient pronominality model:", round(np.mean(y_predicted), 4))
print("Proportion of data with DO outcome:", round(np.mean(dat["Response"]),4))
print(f'Classification Accuracy: {round(accuracy(y_predicted, y_test.values), 4)}')
print(f'Log-Likelihood {round(log_likelihood(y_predicted, y_test), 4)}')

"""The inclusion of both the word length of the theme and recipient has further improved the model. Since the log-transformed lengths slightly outperformed the raw length, we will report the statistics based on these variables. The accuracy has significantly improved to an impressive 0.847, showcasing commendable predictive performance. Additionally, the log-likelihood has increased even further, indicating a substantial enhancement in predictive power and an improved fit. Overall, these covariates are deemed significant and contribute significantly to the model's overall performance.

As previously discussed, the coefficient values indicate that the original variables, recipient and theme pronominiality, hold significant importance in the model. The length variables, on the other hand, do not contribute as substantially. However, we learned in class about Panini's Law and its relationship to Ordering Preferences, where shorter words in terms of syllables tend to take precedence. Although this is not directly applicable to our specific case, there is a connection between word length and number of syllables (albeit imperfect). Intuitively, this connection makes sense, and exploring a model that incorporates this relationship would be intriguing.

Considering the high accuracy achieved without including the length variables, there is a possibility of overfitting. Moreover, incorporating length variables aligns with linguistic sensibility. Additionally, in our case, larger DO phrases are often reported as awkward, suggesting that length may influence the outcome. To integrate these variables into the model, we propose incorporating first-degree interactions between the variables. It is important to note that it would not make sense linguistically to add interactions between variables that pertain to different objects. Therefore, we will introduce the variables 'RecPro & LengthofRecipient' as well as 'ThemePro & LengthofTheme'. Furthermore, we will utilize log-transformed length rather than the raw length, as it yielded a better fit in our case.
"""

#Multiply by log-transformed length
dat['logMultRec'] = dat['RecPro']*dat['logLengthOfRecipient']
dat['logMultTheme'] = dat['ThemePro']*dat['logLengthOfTheme']
N = dat.shape[0]
N_train = math.floor(N*4/5)
idx = list(range(N))
random.seed(3)
random.shuffle(idx)
idx_train = idx[0:N_train]
idx_test = idx[N_train:N]
d_train = dat.iloc[idx_train]
d_test = dat.iloc[idx_test]

x_train = d_train[["logMultRec", "logMultTheme","Dummy"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial())
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["logMultRec", "logMultTheme","Dummy"]]
y_test = d_test[["Response"]]
y_predicted = m_results.predict(x_test)
print("Predicted proportion of DO outcomes based on fitted intercept and recipient pronominality model:", round(np.mean(y_predicted), 4))
print("Proportion of data with DO outcome:", round(np.mean(dat["Response"]),4))
print(f'Classification Accuracy: {round(accuracy(y_predicted, y_test.values), 4)}')
print(f'Log-Likelihood {round(log_likelihood(y_predicted, y_test), 4)}')

"""Given the significant loss of accuracy and goodness of fit observed, it is prudent to explore alternative approaches. Therefore, we will now consider interactions between the two 'Pronominality' variables and the two 'length' variables. This adjustment aims to capture potential synergistic effects between these variables and potentially improve the model's performance."""

#Interactions between the two 'Pronominality' variables and the two 'length' variables
dat['MultRecTheme'] = dat['RecPro']*dat['ThemePro']
dat['MultLength'] = dat['logLengthOfRecipient']*dat['logLengthOfTheme']
N = dat.shape[0]
N_train = math.floor(N*4/5)
idx = list(range(N))
random.seed(3)
random.shuffle(idx)
idx_train = idx[0:N_train]
idx_test = idx[N_train:N]
d_train = dat.iloc[idx_train]
d_test = dat.iloc[idx_test]

x_train = d_train[["MultRecTheme", "MultLength","Dummy"]]
y_train = d_train[["Response"]]
m = sm.GLM(y_train,x_train,family=sm.families.Binomial())
m_results = m.fit()
print(m_results.summary())
x_test = d_test[["MultRecTheme", "MultLength","Dummy"]]
y_test = d_test[["Response"]]
y_predicted = m_results.predict(x_test)
print("Predicted proportion of DO outcomes based on fitted intercept and recipient pronominality model:", round(np.mean(y_predicted), 4))
print("Proportion of data with DO outcome:", round(np.mean(dat["Response"]),4))
print(f'Classification Accuracy: {round(accuracy(y_predicted, y_test.values), 4)}')
print(f'Log-Likelihood {round(log_likelihood(y_predicted, y_test), 4)}')

"""These results are truly impressive. We have achieved a slight improvement in both accuracy and model fit by enhancing the log-likelihood. Moreover, we have accomplished this while reducing the number of variables to only two. This approach, which involves breaking down our variables into separate components representing length and pronominality, is not only statistically beneficial but also linguistically logical. By adopting this approach, we have successfully maintained the essence of the original model while simultaneously enhancing efficiency.

In summary, we have successfully constructed a model that exhibits improved fit and linguistic soundness without sacrificing much accuracy. This was achieved by incorporating first-degree interactions between the relevant variables. By doing so, we have enhanced the model's performance and captured the intricate relationships between the variables more effectively.

##2) Word embeddings

The below code and text are for the second problem on the pset.  Note that the second code chunk will take several minutes to run, but only needs to be run once, which will download the GLoVe vectors and save them on your Google drive in a new folder named *096222-pset-3* (about 1GB for the glove.6B.zip dataset). When done with the pset you may delete the files to free up space.
"""

from google.colab import drive
drive.mount('/content/gdrive')
GDRIVE_DIR = "/content/gdrive/My Drive/096222-pset-3"

# This code chunk needs to be run only the first time through the pset.
# It downloads the GLoVe word embeddings and saves them to your Google drive.
!time wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip
!mkdir -p "$GDRIVE_DIR"
!mv glove.6B.300d.txt "$GDRIVE_DIR/"

import sys
import numpy

def read_vectors_from_file(filename):
    d = {}
    with open(filename, 'rt') as infile:
        for line in infile:
            word, *rest = line.split()
            d[word] = numpy.array(list(map(float, rest)))
    return d

e = read_vectors_from_file(GDRIVE_DIR + "/glove.6B.300d.txt")

e['apples']

"""### Implement and test the cosine measure of word similarity."""

import numpy as np

## Write a function to compute the cosine similarity between two word vectors.
##       Demonstrate that it's symmetric with a few examples.
def cosine_similarity(x: np.ndarray, y: np.ndarray) -> float:
    dot_product = np.dot (x,y)
    x_norm = np.linalg.norm(x)
    y_norm = np.linalg.norm(y)
    res = dot_product/ (x_norm* y_norm)
    return res

def verify(x):
  if x: print("Verified")
  else: print("Failure to verify")

## Use some examples to demonstrate symmetry of your implementation.
verify(cosine_similarity(e['apples'],e['oranges'])==cosine_similarity(e['oranges'],e['apples']))

## TODO: add a few more examples here.
verify(cosine_similarity(e['car'],e['truck'])==cosine_similarity(e['truck'],e['car']))
verify(cosine_similarity(e['mars'],e['venus'])==cosine_similarity(e['venus'],e['mars']))
verify(cosine_similarity(e['warm'],e['cool'])==cosine_similarity(e['cool'],e['warm']))
verify(cosine_similarity(e['red'],e['blue'])==cosine_similarity(e['blue'],e['red']))

## Verify the sanity checks in part 1b of the pset PDF.
verify(cosine_similarity(e['car'],e['truck']) > cosine_similarity(e['car'],e['person']))
verify(cosine_similarity(e['mars'],e['venus']) > cosine_similarity(e['mars'],e['goes'])) # TODO Convert Mars and Venus to lowercase
verify(cosine_similarity(e['warm'],e['cool']) > cosine_similarity(e['warm'],e['yesterday']))
verify(cosine_similarity(e['red'],e['blue']) > cosine_similarity(e['red'],e['fast']))

## TODO: come up with two examples that demonstrate correct similarity relations.
verify(cosine_similarity(e['lion'],e['tiger']) > cosine_similarity(e['lion'],e['car']))
verify(cosine_similarity(e['small'],e['big']) > cosine_similarity(e['small'],e['mars']))

## TODO: come up with two examples where cosine similarity doesn't align with your intuitions about word similarity.
verify(cosine_similarity(e['lion'],e['cat']) > cosine_similarity(e['lion'],e['dog']))
verify(cosine_similarity(e['car'],e['driver']) > cosine_similarity(e['car'],e['truck']))

"""{lion, cat} vs. {lion, dog}:
We believe that "lion" is generally associated with the feline family and therefore would be more strongly associated with "cat", while "dog" is generally associated with animals, a concept that encompasses a wider range of species than just cats.

{car, driver} vs. {car, truck}:
During our comparison of "car" and "driver" versus "car" and "truck," we noted that both pairs are associated with transportation and driving. However, they represent distinct entities and concepts within that domain. "Car" and "truck" are both tangible objects used for transportation, while "driver" represents a concept related to the individual operating the vehicle. This distinction could potentially explain why "car" was found to be more similar to "truck" than to "driver."


"""

## TODO: extra credit goes here if you want to do it.
def euclidean_distance(x: np.ndarray, y: np.ndarray):
  sqrt = np.square(x - y)
  sum = np.sum(sqrt)
  res = np.sqrt(sum)
  return res

#Comparing of the same pair of words of the cosine similarity
verify(euclidean_distance(e['apples'],e['oranges'])==euclidean_distance(e['oranges'],e['apples']))
verify(euclidean_distance(e['car'],e['truck']) > euclidean_distance(e['car'],e['person']))
verify(euclidean_distance(e['mars'],e['venus']) > euclidean_distance(e['mars'],e['goes']))
verify(euclidean_distance(e['warm'],e['cool']) > euclidean_distance(e['warm'],e['yesterday']))
verify(euclidean_distance(e['red'],e['blue']) > euclidean_distance(e['red'],e['fast']))

## TODO: come up with two examples that demonstrate correct similarity relations.
#The same test
verify(euclidean_distance(e['lion'],e['tiger']) > euclidean_distance(e['lion'],e['car']))
verify(euclidean_distance(e['small'],e['big']) > euclidean_distance(e['small'],e['mars']))

## TODO: come up with two examples where cosine similarity doesn't align with your intuitions about word similarity.
#The same test
verify(euclidean_distance(e['lion'],e['cat']) > euclidean_distance(e['lion'],e['dog']))
verify(euclidean_distance(e['car'],e['driver']) > euclidean_distance(e['car'],e['truck']))

"""Our observation revealed that for every "Verified" pair of similarity scores calculated using cosine similarity, we encountered "Failure to verify" pairs when employing Euclidean distance, and vice versa. This discrepancy can be attributed to the fundamental distinction in how these two metrics capture similarities between vectors. Cosine similarity focuses on measuring the similarity in direction or orientation of two vectors, while Euclidean distance considers both the similarity in magnitude and direction of the difference between them. Consequently, vectors with similar orientations but different magnitudes may be deemed similar by cosine similarity but dissimilar by Euclidean distance, resulting in contrasting similarity outcomes.

### The analogies task

Given words *w1*, *w2*, and *w3*, find a word *x* such that *w1* : *w2* :: *w3* : *x*. For example, for the analogy problem *France*:*Paris* :: *England*:*x*, the answer should be *London*. To solve analogies using semantic vectors, letting $e(w)$ indicate the embedding for a word w, calculate a vector $y$ = $e(w_2)$ − $e(w_1)$ + $e(w_3)$ and find the word whose vector is closest to $y$.

**TODO:** Explain why the analogy-solving method makes sense.

The analogy-solving method is logical because it utilizes word embeddings that are based on the inherent semantic relationships encoded within word vectors. Word embeddings assign words to high-dimensional vectors in a semantic space, where words with similar meanings are situated closer to each other compared to words with dissimilar meanings. These spatial relationships between vectors effectively capture the semantic connections between words.

By computing a vector that encapsulates the semantic relationship between two given words in an analogy, we can utilize this vector to predict an unknown word that shares a similar relationship to the third word. This prediction is accomplished by identifying the word whose vector is closest to the computed vector, utilizing a distance metric such as cosine similarity or Euclidean distance.
"""

## Write a function to calculate y as described above.
def analogy_vector(w1: str, w2: str, w3: str, e: dict) -> np.ndarray: # note that the function takes the word embedding dictionary as input.
  y = e[w2]- e[w1] + e[w3]
  return y

## Write a function to find the k nearest neighbors to y.
def analogy(w1: str, w2: str, w3: str, e: dict, k=5):
  y= analogy_vector(w1, w2, w3, e)
  nn= {} #nearest neighbors
  top_keys = []

  for word in e:
     nn[word] = cosine_similarity(y, e[word])
  nn = dict(sorted(nn.items(), key=lambda item: item[1], reverse= True))

  for i, key in enumerate(nn.keys()):
    if key!= w3:
      top_keys.append(key)
    if i == k:
        break

  return top_keys

## Are the top 5 results for the following analogies sensible?
print(analogy("france","paris","england",e))
print(analogy("man","woman","king",e))
print(analogy("tall","taller","warm",e))
print(analogy("tall","short","england",e))

"""
It's good to see that most of the results make sense. Regarding the fourth analogy, it seems like the model may be struggling with finding an appropriate analogy since there isn't really an opposite of a county. It's possible that the model is picking up on other associations with the word "county", such as location or population size, which might explain why words like "city" and "following" are appearing in the results. Nonetheless, it's interesting to see how the model is attempting to find a relationship between the given words.

Let's analyze each analogy individually:

In the first analogy, "London" being the closest word to the vector "y" is as we anticipated. Additionally, all the other four words being cities is in line with our understanding.

Moving to the second analogy, "queen" being the closest word to the vector "y" is the correct analogy. We can observe that the other four words are related either to strong women or to monarchy, which is conceptually relevant.

In the third analogy, "warmer" being the closest word to the vector "y" indicates that the algorithm successfully recognizes similar semantic contexts. The presence of other words related to "cold" and "warm" is logical given the given words and context.

For the fourth analogy, the absence of an exact analogy is understandable since "tall" and "short" are antonyms, and there is no direct opposite for a county. Consequently, the appearance of other cities and the word "short" in the results can be considered reasonable. However, it is unclear why the word "following" is one of the closest words, as it has a different meaning.

Overall, while most of the results align with our expectations and make sense, there may be instances where further examination is needed to understand the associations made by the algorithm."""

## TODO: come up with 4 more analogies, 2 of which work in your opinion, and 2 of which don't work.

#Some of the examples were taken from Psychometric exams, for the sport :)

#analogies which we believe work
print(analogy("mother","father","girl",e))
print(analogy("small","smaller","big",e))

#analogies which we believe won't work
print(analogy("house","person","kennel",e))
print(analogy("drink","water","play",e))

"""**TODO:**
Did you notice any patterns or generalizations while exploring possible analogies? For the ones that went wrong, why do you think they went wrong?

We agree that obtaining accurate results in analogies often requires precise alignment between the words. The presence of plural, comparative, and superlative forms of words can pose challenges for the algorithm to accurately predict appropriate contexts and analogies.

Let's examine the examples provided:

1) Paris is the **capital** of France, just as London is the **capital** of England.

2) "smaller" is the comparative form of the adjective "small", just like "big" and "bigger". Similar to the given example of "tall":"taller" :: "hot": "warmer".
We can see that the algorithm is capable of these words correctly.

3) "mother" and "father" are identified as male and female, just as "girl" and "boy" are identified as male and female.

For the examples that didn't work, the relationship between the words is a bit more complex:

1) A house is a **living place for humans**, while a kennel is a **living place for dogs**. We believe that the complication in understanding the context detracts from the algorithm's ability to understand the intent. That is, if the context is not direct, or alternatively does not point directly to the reason for the context - the algorithm will not be accurate.

2) We believed that since the act of drinking is related to **water**, the act of playing would be related to **game** and we would get the correct analogy. However, the best word we got was "water". In addition, the results also include variations of "play", which supports our previous conclusion. We believe that the analogy is also a bit complicated because you don't just drink water, and there are other things that can be played, and not just a game.

Indeed, complex analogies can pose challenges for the algorithm, even if they appear logical and structured to human thinking. While the algorithm excels at capturing certain linguistic patterns and relationships, it may struggle with more intricate contexts that require deeper understanding or contextual knowledge.

Analogical reasoning often relies on the ability to identify subtle similarities and abstract relationships between words. Human thinking is influenced by various factors, including background knowledge, cultural context, and personal experiences, which allow us to make connections that may not be apparent solely from word embeddings.

The algorithm's performance is limited to the information it has been trained on and the patterns it has learned from the data. It may struggle to grasp the intricacies of certain analogies that require additional contextual understanding beyond the surface-level associations present in the word embeddings.

While the algorithm may fall short in complex scenarios, it is important to acknowledge its capabilities in recognizing more straightforward analogies and capturing certain linguistic patterns accurately. Continued research and advancements in natural language processing aim to improve the algorithms' ability to handle more complex analogies and align them more closely with human reasoning.

##3) Using semantic vectors to decode brain activation

### Load the data
"""

# Download and extract the data and learn_decoder.py
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xZaorRH-xxjfochvSesAhOTUg82_Xq56' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1xZaorRH-xxjfochvSesAhOTUg82_Xq56" -O files.zip && rm -rf /tmp/cookies.txt
!unzip files.zip
!rm files.zip

#Let's load the functions from learn_decoder.py
from learn_decoder import *

#and the data
data = read_matrix("imaging_data.csv", sep=",")
vectors = read_matrix("vectors_180concepts.GV42B300.txt", sep=" ")
concepts = np.genfromtxt('stimuli_180concepts.txt', dtype=np.dtype('U')) #The names of the 180 concepts

"""You can verify for your self what learn_decoder consists of by going to Files and opening it.

### What are the Accuracy scores?

Define a function that computes rank-based accuracy score, then, iterate over the 18 folds. For each fold,  train the decoder **using the `learn_decoder` function** (the function is already imported from `learn_decoder.py`) on the fold train data, obtain the predicions on the fold test data, and store both the accuracy score of each concept (use the labels from `concepts`) as well as the average score of the 10 concepts.
"""

#calculate the rank of the true vector based on cosine similarity
def rank_based_accuracy (decoded_vec, true_vec):
  rank= {}
  true_vec_index = -1

  for i in range(vectors.shape[0]):
    rank[i]= cosine_similarity(vectors[i], decoded_vec)
    if np.array_equal(true_vec, vectors[i]):
      true_vec_index=i

  rankings = dict(sorted(rank.items(), key=lambda item: item[1], reverse= True))
  final_rank = list(rankings).index(true_vec_index)

  return final_rank+1

all_ranks = {} #rank of each concept
avg_ranking = {} #avg rank of each fold

def calculate_rank(decoder_res, test_set, test_vectors, index):
  key= (index/10)+1
  test_fold_ranking= []
  for i in np.arange(test_set.shape[0]):
    dot_prod = np.dot(test_set[i],decoder_res) #semantic vector - the model's best guess to the concept
    current_rank= rank_based_accuracy(dot_prod, test_vectors[i]) #rank of the current test data
    test_fold_ranking.append(current_rank)
    all_ranks[concepts[index]]= current_rank
    index= index+1

  avg_ranking[key]= sum(test_fold_ranking)/len(test_fold_ranking)

#iterate each fold and split to train set and test set

for i in np.arange(0, data.shape[0], 10):
  temp_test_set= data[i:i+10]
  temp_test_vectors= vectors[i:i+10]

  if i ==0:
    temp_train_set = data[i+10:data.shape[0]]
    temp_train_vectors= vectors[i+10:data.shape[0]]
  elif i== 170:
    temp_train_set= data[0: data.shape[0]-10]
    temp_train_vectors= vectors[0: data.shape[0]-10]
  else :
    temp_train_set= np.concatenate((data[0:i], data[i+10: data.shape[0]]))
    temp_train_vectors= np.concatenate((vectors[0:i], vectors[i+10: data.shape[0]]))

  decoder_res= learn_decoder(temp_train_set, temp_train_vectors) #decoder matrix

  calculate_rank(decoder_res, temp_test_set, temp_test_vectors, i) #calculate rank for each concpet and for each fold

  print(f'average score of fold num {(i/10)+1} is: {avg_ranking[(i/10)+1]}')

"""Now let's plot the averaged accuracy score for each fold  """

import matplotlib.pyplot as plt

x_axis =np.arange(1,19)
y_axis = avg_ranking.values()
plt.scatter(x_axis, y_axis)
x_tick = plt.xticks(range(min(x_axis), max(x_axis)+1))
plt.title("Accuracy Score per Fold")
plt.xlabel("Number of Fold")
plt.ylabel("Avg Accuracy Score")

"""### Which concepts can be decoded with more or less success?

We'll consider a "successful concept"  as a concept with an average rank lower than 90, as a score of 90 represents random noise.
"""

successful_concepts = {}
failed_concepts = {}

for concept in concepts:
  if all_ranks[concept]<90:
    successful_concepts[concept]= all_ranks[concept]
  else:
    failed_concepts[concept]= all_ranks[concept]

successful_concepts = dict(sorted(successful_concepts.items(), key=lambda item: item[1]))
failed_concepts = dict(sorted(failed_concepts.items(), key=lambda item: item[1] ,  reverse= True))

print(f'successful concepts are : {successful_concepts}')

print(f'failed concepts are : {failed_concepts}')

#Top 15 successful concepts
import itertools

top_15 = dict(itertools.islice(successful_concepts.items(), 15))
x_axis =top_15.keys()
y_axis = top_15.values()
plt.figure(figsize=(10,6))
plt.bar(x_axis, y_axis)
x_tick = plt.xticks(rotation = 60)
plt.title("Top 15 Successful Concepts")
plt.xlabel("Concept")
plt.ylabel("Accuracy Score")
plt.plot()

#Top 15 failed concepts

top_15 = dict(itertools.islice(failed_concepts.items(), 15))
x_axis =top_15.keys()
y_axis = top_15.values()
plt.figure(figsize=(10,6))
plt.bar(x_axis, y_axis)
x_tick = plt.xticks(rotation = 60)
plt.title("Top 15 Failed Concepts")
plt.xlabel("Concept")
plt.ylabel("Accuracy Score")

"""### Are the results satisfactory, in your opinion? Why or why not?

\#TODO

# Export to PDF

Run the following cell to download the notebook as a nicely formatted pdf file.
"""

# Add to a new cell at the end of the notebook and run the follow code,
# which will save the notebook as pdf in your google drive (allow the permissions) and download it automatically.

!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py

from colab_pdf import colab_pdf

# If you saved the notebook in the default location in your Google Drive,
# and didn't change the name of the file, the code should work as is.
# If not, adapt accordingly.

colab_pdf(file_name='Copy of Pset_3.ipynb', notebookpath="/content/drive/MyDrive/Colab Notebooks/")

from gensim.test.utils import common_texts
from gensim.models import Word2Vec

model = Word2Vec(sentences=concepts, vector_size=100, window=5, min_count=1, workers=4)
model.save("word2vec.model")